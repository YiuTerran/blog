---
title: 边缘应用托管方案设计
description:
toc: true
authors: ["tryao"]
tags: ["edge","design"]
categories: []
series: []
date: 2023-02-22T09:12:59+08:00
lastmod: 2023-02-22T09:12:59+08:00
featuredVideo:
featuredImage:
draft: true
---

## 需求与现状

我们需要一套方案，来解决边侧应用部署的问题。这不仅仅包括边缘网关，同时还应包括独立部署整个iot平台，以及任意第三方服务。当然，除了部署之外，还有**权限隔离**、**运行监测**（可观测性）和**远程调试**等常见的需求。目前来说，我们**不需要**考虑边缘端服务器配置轻量化需求，所以后面略去了这部分的考量。

目前，IoT云测服务运行于阿里云托管的k8s中，相关权限在运维手中。跟院里大部分服务一样，IoT整体架构并不强依赖于k8s，而是使用了SpringCloud等框架，换句话来说，并没有云原生化。特别地，视频云服务使用了大量UDP based（RTP/SIP）协议，而k8s对于非TCP based协议的网络支持都比较有限，短时间内恐怕很难on k8s. 所以，**我们必须同时支持k8s部署和普通部署**。

我们可以强制要求所有的应用运行在**容器**中，通过合理规划端口使用和卷挂载，理论上可以解决绝大部分服务运行问题。但是第三方应用（如视频AI之类的）可能不支持容器化，当然这种情况也可以让他们自己部署。那么，我们的方案最好的情况需要支持三级部署：裸程序、容器化和k8s里，次一级也至少支持容器化和k8s两种。

边侧服务除掉最早期使用windows server的版本，目前都运行在一期基于k3s的应用托管容器中。目前的应用托管（以下简称v1）架构比较简单：

![06508ac1f18f5a725dde24846f37f753-20230218195750](https://csceciti-iot-devfile.oss-cn-shenzhen.aliyuncs.com/docs/06508ac1f18f5a725dde24846f37f753-20230218195750.png)

云端是一个rancher集群，边侧就是一个k3s集群（类似于一个k8s集群），云边之间有建立websocket通道，而各边侧之间是相互隔离的（当然可以通过云端rancher中转通信，不过这个不是out-of-box的）。该方案对云端有没有k8s没有需求。

墨斗通过调用rancher的各种API，来与pod中的服务进行交互，从而可以达到远程调试的目的。

通过额外在k3s中部署MetricServer，Prometheus等服务，也可以将数据采集并上报到云端。当然这个问题需要细化讨论，第三方托管服务的可观测性可能需要他们自己想办法解决，或者我们提供一套SDK供他们收集日志之类的。

此外，rancher也提供了较为完善的API做权限隔离。所以虽然目前还做的比较简陋，但是显然该方案可以解决**所有服务都能跑在k8s中的需求**。

## 统一托管方案

### 裸机部署

裸程序部署方案，在k8s甚至docker出现之前就有很多，但是大都已经是明日黄花。目前社区中尚有人气的主要是[hashicorp](https://github.com/hashicorp)/**[nomad](https://github.com/hashicorp/nomad)**，该方案使用了与k8s编排容器完全不同的抽象方式，从而能够部署非容器化应用，该方案的问题也是它**并不支持**k8s生态。换句话说，一台裸Linux上可以同时存在docker和k8s和普通二进制服务，但是不能同时存在nomad和k8s，所以除非全面倒向nomad生态，不然一般是不考虑使用的。

我们把要求放低一点，如果是裸机的普通控制，实际上**并不需要考虑部署**的事情，只要给用户一个远程交互的界面，其他的让他自己处理就行。就像向日葵这种远程桌面一样，理论上远程机器上有一个websocket的agent就可以通过web进行远程管理了。这样的工具还是有一些的，如[rport](https://github.com/cloudradar-monitoring/rport)，该开源项目同时支持linux/windows/mac，并支持X86/ARM等多种架构（这是因为agent是用go写的）。功能上支持**远程桌面**，也支持**webshell**，整体有点像阿里云ECS的控制台。

另外值得一提的是，该项目的部署方案十分完善，提供了大量自动化脚本应对各种情况。

![dashboard-with-client-inventory](https://csceciti-iot-devfile.oss-cn-shenzhen.aliyuncs.com/docs/dashboard-with-client-inventory.jpg)

上面是一张截图，不过我们实际上应该只会通过API与之交互。

裸机部署很难统一应用交付方式，服务一般是通过各种脚本来启动。一般也不考虑环境隔离的问题，可以通过windows/linux创建非管理员账户来限制租户的权限，总之都是比较传统的方案。

特别需要注意的是，裸机方案不仅适配于应用托管，在更小型的环境里（如树莓派或者更低的配置）运行应用程序都应当考虑该方案。因为docker启动之后至少需要200~300M内存，k3s更是需求1G内存。而且，**即使是采用了k8s方案，对于内网的宿主机，我们还是需要有远程访问的能力，方便进行运维管理**。

此外，虽然很少，但是对于某些强依赖windows环境的应用，也只能采用裸机部署方案。

### 容器部署

其实rancher1.x的时候就支持docker swarm的裸机部署，不过rancher从2.0开始就只支持k8s了。

目前流行的开源方案中，[portainer](https://www.portainer.io/)是功能比较全面的容器环境管理系统，它不仅支持docker compose/k8s，甚至还支持前面提到的namod方案。

![portainer-architecture-detailed](https://csceciti-iot-devfile.oss-cn-shenzhen.aliyuncs.com/docs/portainer-architecture-detailed.png)

这个是它的架构图。左侧是边缘Agent，右侧是云端Agent. 该服务最开始的架构是pull模型，EdgeAgent在内网，只能改成push模型。

与想象的不同，该方案的Agent与Server是按需连接的。EdgeAgent会每隔几秒（默认是5秒）探测一下服务端是否需要建立长连接，需要的话才会建立websocket通道。所以Edge与Server的通信建立可能有一定的延迟。

![image-20230223173543309](https://csceciti-iot-devfile.oss-cn-shenzhen.aliyuncs.com/docs/image-20230223173543309.png)

上面是该服务运行时截图。

经过测试，portainer可以比较完美的支持容器部署的常用功能，并能直接透传docker命令。k8s的支持稍弱，不过也能使用helm部署，和自定义API对象模板，足够满足一般使用需求。

对于应用部署，docker环境提供了简单的表单方案，当然支持直接上传compose file；k8s环境也支持通过表单简单部署ReplicaSets. 当然k8s环境可以使用其他部署方案，不一定要走他这一套，后面会详述。

有一定的权限隔离支持，但是不太完善（RBAC属于收费功能），可以通过前端封装API来解决相关问题。

总体来说，`portainer`配合docker和k3s集群，可以解决大部分应用部署的问题。需要注意的是，portainer的官方地址疑似被墙，安装脚本虽然完善，但是需要改动才能使用（当然k8s本身就被墙了……）。

### k8s部署

这个可选的方案就很多了，目前在用的k3s+rancher也是比较成熟的方案，当然也可以用上面说的portainer+k3s替代。除此之外还有KubeEdge/OpenYurt等边缘计算的方案，下面对这些方案做一个简单对比。

#### k3s+rancher/portainer

前文已经说过这种架构，v1版本用rancher2.x配合k3s可以解决大部分k8s内部署的问题，k3s对k8s做了精简，并且自带了LocalPV和Ingress的配置，开箱即用。不过rancher没有官方的API指南，需要自行抓包看API，这点不如portainer; rancher的优势是用户更多，知名度更大，这样理论上会更加稳定一些。

这种架构最大的特点，是服务端并不需要k8s集群。这有利有弊，好处当然是比较轻量封闭；坏处是无法进行云边算力协同，边与边之间的集群也没有任何关系。如果是完全云原生化的服务，分区域部署，显然不能使用该方案。

不过对于目前的墨斗平台而言，使用该方案**无迁移成本**，反而是最优选择。

另外值得一提的是，k3s部署非常简单，有官方的中国区镜像，整个过程可以在线完成，无须翻墙。

#### k8s+KubeEdge

![KubeEdge Architecture](https://csceciti-iot-devfile.oss-cn-shenzhen.aliyuncs.com/docs/kubeedge_arch.png)

主要组件：

1. 云端的cloudCore运行在k8s中，包括：
   1. CloudHub是一个websocket服务，负责与边缘端的EdgeHub通信；
   2. EdgeController，负责桥接k8s-apiserver与edgecore. 即所有在云端通过kubectl管理边缘端API对象的行为，都是通过它来转达的；通过apiserver的webhook功能实现；
   3. DeviceController. Device在KubeEdge中被定义为一种CRD. 所以也可以通过`kubectl`来进行管理。这种设计实际上是将**业务耦合进基础设施中**，或者也可以说是完全云原生化；**这种设计看起来很方便，但是实际使用中有很大的问题**；
2. 边缘端的EdgeCore是一套简化的k8s（与k8s生态不兼容，甚至无法运行kubectl命令）；主要包括：
   1. EdgeHub，与CloudHub对应的websocket客户端，负责与云端进行通信；所有其他边缘组件都通过它与云端进行通信；
   2. Edged，简化的kubelet，负责边缘端容器编排；
   3. MetaManager，类似ETCD的元数据管理（使用sqlite）；
   4. DeviceTwin，设备孪生（或者设备影子）；可以理解为一种缓存，用于同步设备状态；
   5. EventBus，一个mqtt消息总线，订阅mqtt broker的消息，并转发给edgehub，后者再同步给云端（反过来的路径也是成立的）；
   6. ServiceBus，类似EventBus，一个http消息总线，负责与边缘端运行的http服务进行交互；可以解决需要反向拉取HTTP数据的场景；
   7. Mapper, 一种协议抽象方式。遵从kubeedge的物模型规范，可以将协议实现为一个小的**golang app**，该app一般作为一个deployment在k8s中进行部署；

可以看出，kubeEdge是一个边缘计算的**打包**方案，主要解决了几个问题：

1. 容器远程管理。可以在云端直接部署边缘端的服务，通过nodeAffinity等方式将服务部署到合适的边缘端；可以在云端统一管理边缘容器；
2. 统一消息路由。应用层不需要关心消息如何传递的，云端服务只需和cloudHub通信，边缘端Mapper只需和Mqtt broker通信；
3. 统一设备抽象。提供了统一的物模型设计，统一的驱动（协议）开发方式；包括统一的设备影子设计，用于同步设备状态；
4. 优化资源占用和满足离线自治。这也是其他方案都有的能力；

**存在的问题**：

1. 统一设备抽象并不好。按着目前的设计，大量数据会被存储在云端etcd里（因为每个设备相当于是一个configMap），对于超大规模物联网平台而言，这个数据量会非常大。etcd搞增删改查的性能堪忧；
2. 统一消息路由问题同样很大。首先是云端直连场景实际上并没有考虑，然后客户端的场景也只考虑了HTTP，实际上协议支持非常不完善。然后就是性能问题，目前这套设计消息会经过多个实体，最终才能到达云端服务，延迟必然收到影响。且在边缘端连接大量设备时，消息总线能支持多大的量还是未知；
3. 目前仅支持golang mapper，这对使用其他语言技术栈的公司很不友好；
4. 开发门槛提高。由于业务深入耦合k8s，完全云原生化，这就要求相关开发必须深入理解k8s的原理，不然除了问题很难调试；
5. 部署非常麻烦。kubeEdge提供的安装工具实际上都必须翻墙才能正常使用，这就导致这个部署流程非常麻烦，而且不够稳定；
6. 文档稀烂，资料较少；虽然是华为主导开发，但是中文文档写的还不如k3s，英文文档版本也无法跟得上release, 出现问题比较难找到资料；
7. 边缘端与k8s生态不兼容；只有少量开源项目支持kubeEdge，如kubesphere. 一般开源软件只兼容原生k8s. 边缘端实际上无法自治，在离线情况下，边缘端由于缺少kubectl工具，实际上很多功能无法使用，比如离线部署应用；

经过实际调研，使用KubeEdge的用户大部分还是只用它最基础的容器远程管理和部分网络功能，IoT部分还是走自研的消息通路。实际上github上Mapper项目的star数还不足100…

#### k8s+OpenYurt方案

KubeEdge的问题实际上是过渡设计了，如果只需要保留最基本的网络拓扑和容器管理功能，可以考虑使用OpenYurt.

这个方案和KubeEdge实际上不太一样，它主要解决的其实是跨机房集群问题，比如多个异地机房组成服务集群这种场景。所以在设备管理、以及轻量化这两方面，做的不如kubeEdge.

![img](https://openyurt.io/zh/assets/images/arch-3dc62565787cc0f579dd62eba788f49a.png)



上图是OpenYurt的架构。包括以下通用组件：

* YurtHub：同样是流量代理，sidecar模式，同时支持云端和边缘端两种运行模式；
* Raven：打通云端和边缘端的网络流量，可以理解成一个VPN；

云端额外有**YurtControllerManager**和**YurtAppManager**，负责证书、调度等工作，用户一般无需关心；

边缘端额外有**Pool-Coordinator**，负责心跳管理，边缘端运维监控等功能。

OpenYurt主要是抽象了节点池的概念，我们可以简单的把一个边缘机房里的所有服务器理解为一个节点池。OpenYurt主要是方便云端统一管理各个节点池的资源，统一进行部署运维等工作；并能协调边云资源，解决负载不均衡造成的资源浪费问题。

优点：

* 与业务没有耦合，解决问题比较单一，比较容易理解；
* 为应用部署做了节点池等抽象，有比较完整的部署模型。这点KubeEdge没有；
* 中文文档完整，边缘部署比较方便（基本无须翻墙），云端部署缺乏一键脚本；
* 阿里云有个商业版本的Edge@Ack，两者设计几乎一致；
* 与k8s生态兼容（无论边云，本质上都还是k8s）；

缺点：

* 成熟度不如kubeEdge，目前案例较少；
* 大规模使用的性能未知，官方暂未给出相关数据（仅有100节点的测试报告）；kubeEdge有10w边缘节点的数据（但是没有多设备压测的数据）；

#### 总结

虽然还有superEdge等开源产品，不过成熟度不够高，这里不再赘述。

不管哪个方案，都应该有一个宿主机管理工具（除非宿主机被映射到了公网，可以直接ssh连接），这里假设用rport；

不管哪个方案，都应该有一套监控工具。这里仍然统一使用之前提的ElasticAgent + Elastic APM方案即可，支持所有环境（k8s/非k8s）的可观测性数据收集。当然，如果要集成到墨斗上，需要自己开发API与es进行交互，否则就直接通过kibana看就行；

保守方案1：同时支持裸机/docker/k8s，那么用rport+portainer+docker+k3s，可以解决所有问题；我方服务主要和rport/portainer交互；

一般方案：仅支持k8s，使用rport+云端k8s+边缘OpenYurt/KubeEdge，其中kubeEdge仅使用边云打通功能，不使用iot相关功能；云端单独部署k8s master节点与edge通信，墨斗iot仍然部署在阿里云的托管集群里；我方服务直接和云端的k8s apiserver交互。

保守方案2：在一般方案的基础上，增加portainer，只负责docker交互；

激进方案：使用KubeEdge的全部功能，重构墨斗平台，使其完全云原生化（工具量非常大，网关层的所有协议，hermes的设备管理，以及视频云信令服务都要重构，且需要使用go语言）。需要将云端墨斗移入单独的k8s环境，与边缘端打通。当然，如果需要完全云原生化，还可以进一步的，抛弃springCloud框架，使用dapr/istio等云原生方案；

## 交互简化方案

k8s对大部分开发人员来说过于复杂，直接填写yaml的方式经常容易出错。因此如何进一步简化部署难度，减轻开发人员压力，也是一个值得讨论的话题。

首先，前面提到的portainer方案，自带了一些简化部署的表单功能，可以参考。

然后，k8s场景，[OAM（Open Application Model）](https://oam.dev/)是阿里巴巴和微软共同开源的云原生应用规范模型，使用该模型可以分清开发和运维的责任，减轻开发使用k8s的难度。

![OAM 的原理](https://csceciti-iot-devfile.oss-cn-shenzhen.aliyuncs.com/docs/oam-principle.jpg)

上图是其工具原理，可以看到实际上是在k8s前面又加了一层。

kubeVela项目是OAM yaml的解析器，结合kubeVela项目和OpenYurt项目，可以比较好的解决应用部署/应用市场的实现难度。

![first-app-graph](https://csceciti-iot-devfile.oss-cn-shenzhen.aliyuncs.com/docs/first-app-graph.jpg)

当然，kubeVela本身也比较复杂，增加了许多概念需要理解。

具体可以参考[这里](https://kubevela.io/zh/blog/2023/01/09/kubevela-openyurt-integration)，有一些介绍。当然，由于kubeEdge的边缘端并不兼容k8s生态，这个方案理论上是无法在kubeEdge上跑的。

## 应用托管功能设计

