<!DOCTYPE html>
<html lang='zh' dir='ltr' ><meta charset="utf-8">
<meta name="viewport" content="width=device-width">


<title>K8s要点简记 | 应许之地</title>

<meta name="generator" content="Hugo Eureka 0.9.0" />
<link rel="stylesheet" href="https://yiuterran.github.io/blog/css/eureka.min.css">
<script defer src="https://yiuterran.github.io/blog/js/eureka.min.js"></script>

<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="preload"
  href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600;700&family=Noto+Serif+SC:wght@400;600;700&display=swap"
  as="style" onload="this.onload=null;this.rel='stylesheet'">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/styles/solarized-light.min.css"
   media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/highlight.min.js"
   crossorigin></script>

  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/languages/dart.min.js"
     crossorigin></script>

<script defer src="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/js/all.min.js"
   integrity="sha256-uNYoXefWRqv&#43;PsIF/OflNmwtKM4lStn9yrz2gVl6ymo="  crossorigin></script>




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
   integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3&#43;Aro6EYUG4&#43;cU&#43;KJWu/X"  media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" 
  integrity="sha384-g7c&#43;Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI&#43;sEnkvrMWph2EDg4"  crossorigin></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
   integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC&#43;Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa"  crossorigin></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false },
        { left: "\\(", right: "\\)", display: false },
        { left: "\\[", right: "\\]", display: true }
      ],
    });
  });
</script>


<script defer src="https://cdn.jsdelivr.net/npm/mermaid@8.9.2/dist/mermaid.min.js" 
  integrity="sha256-Zmpaaj&#43;GXFsPF5WdPArSrnW3b30dovldeKsW00xBVwE="  crossorigin></script>
<link rel="preconnect" href="https://www.google-analytics.com" crossorigin>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-C0R8DENDJ0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());
  gtag('config', 'G-C0R8DENDJ0');
</script>


<link rel="icon" type="image/png" sizes="32x32" href="https://yiuterran.github.io/blog/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_32x32_fill_box_center_3.png">
<link rel="apple-touch-icon" sizes="180x180" href="https://yiuterran.github.io/blog/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_180x180_fill_box_center_3.png">

<meta name="description"
  content="22年的时候需要使用CICD的时候，看了一遍k8s相关的知识点。买了一本《深入剖析Kubernetes》，大致熟悉了常用的概念。 最近因为工作调动需要做">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
      "@type": "ListItem",
      "position": 1 ,
      "name":"文章列表",
      "item":"https://yiuterran.github.io/blog/posts/"},{
      "@type": "ListItem",
      "position": 2 ,
      "name":"K8s要点简记",
      "item":"https://yiuterran.github.io/blog/posts/k8s%E8%A6%81%E7%82%B9%E7%AE%80%E8%AE%B0/"}]
}
</script>



<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://yiuterran.github.io/blog/posts/k8s%E8%A6%81%E7%82%B9%E7%AE%80%E8%AE%B0/"
    },
    "headline": "K8s要点简记 | 应许之地","datePublished": "2023-02-10T09:30:19+08:00",
    "dateModified": "2023-02-10T09:30:19+08:00",
    "wordCount":  13833 ,
    "author": {
        "@type": "Person",
        "name": ["tryao"]
    },
    "publisher": {
        "@type": "Person",
        "name": "C. Wang",
        "logo": {
            "@type": "ImageObject",
            "url": "https://yiuterran.github.io/blog/images/icon.png"
        }
        },
    "description": "22年的时候需要使用CICD的时候，看了一遍k8s相关的知识点。买了一本《深入剖析Kubernetes》，大致熟悉了常用的概念。 最近因为工作调动需要做"
}
</script><meta property="og:title" content="K8s要点简记 | 应许之地" />
<meta property="og:type" content="article" />


<meta property="og:image" content="https://yiuterran.github.io/blog/images/icon.png">


<meta property="og:url" content="https://yiuterran.github.io/blog/posts/k8s%E8%A6%81%E7%82%B9%E7%AE%80%E8%AE%B0/" />




<meta property="og:description" content="22年的时候需要使用CICD的时候，看了一遍k8s相关的知识点。买了一本《深入剖析Kubernetes》，大致熟悉了常用的概念。 最近因为工作调动需要做" />




<meta property="og:locale" content="zh" />




<meta property="og:site_name" content="应许之地" />






<meta property="article:published_time" content="2023-02-10T09:30:19&#43;08:00" />


<meta property="article:modified_time" content="2023-02-10T09:30:19&#43;08:00" />



<meta property="article:section" content="posts" />


<meta property="article:tag" content="k8s" />





<body class="flex flex-col min-h-screen">
  <header class="fixed flex items-center w-full min-h-16 ps-scrollbar z-50 bg-secondary-bg shadow-sm">
    <div class="w-full max-w-screen-xl mx-auto"><script>
    let storageColorScheme = localStorage.getItem("lightDarkMode")
    if (((storageColorScheme == 'Auto' || storageColorScheme == null) && window.matchMedia("(prefers-color-scheme: dark)").matches) || storageColorScheme == "Dark") {
        document.getElementsByTagName('html')[0].classList.add('dark')
    }
</script>
<nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0">
    <a href="https://yiuterran.github.io/blog/" class="me-6 text-primary-text text-xl font-bold">应许之地</a>
    <button id="navbar-btn" class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
        <i class="fas fa-bars"></i>
    </button>

    <div id="target"
        class="hidden block md:flex md:grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20">
        <div class="md:flex md:h-16 text-sm md:grow pb-4 md:pb-0 border-b md:border-b-0">
            <a href="https://yiuterran.github.io/blog/authors/tryao/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">作者</a>
            <a href="https://yiuterran.github.io/blog/posts/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  selected-menu-item  me-4">文章</a>
        </div>

        <div class="flex">
            <div class="relative pt-4 md:pt-0">
                <div class="cursor-pointer hover:text-eureka" id="lightDarkMode">
                    <i class="fas fa-adjust"></i>
                </div>
                <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id="is-open">
                </div>
                <div class="absolute flex flex-col start-0 md:start-auto end-auto md:end-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40"
                    id='lightDarkOptions'>
                    <span class="px-4 py-1 hover:text-eureka" name="Light">浅色</span>
                    <span class="px-4 py-1 hover:text-eureka" name="Dark">深色</span>
                    <span class="px-4 py-1 hover:text-eureka" name="Auto">自动</span>
                </div>
            </div>
        </div>
    </div>

    <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id="is-open-mobile">
    </div>

</nav>
<script>
    
    let element = document.getElementById('lightDarkMode')
    if (storageColorScheme == null || storageColorScheme == 'Auto') {
        document.addEventListener('DOMContentLoaded', () => {
            window.matchMedia("(prefers-color-scheme: dark)").addEventListener('change', switchDarkMode)
        })
    } else if (storageColorScheme == "Light") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'sun')
        element.firstElementChild.classList.add('fa-sun')
    } else if (storageColorScheme == "Dark") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'moon')
        element.firstElementChild.classList.add('fa-moon')
    }

    document.addEventListener('DOMContentLoaded', () => {
        getcolorscheme();
        switchBurger();
    });
</script>
</div>
  </header>
  <main class="grow pt-16">
    <div class="ps-scrollbar">
      <div class="w-full max-w-screen-xl lg:px-4 xl:px-8 mx-auto">


<div class="grid grid-cols-2 lg:grid-cols-8 gap-4 lg:pt-12">
    <div
        class="col-span-2  lg:col-span-6 bg-secondary-bg rounded px-6 py-8">
        <h1 class="font-bold text-3xl text-primary-text">K8s要点简记</h1>
        <div class="flex flex-wrap flex-row items-center mt-2 text-tertiary-text">
    <div class="me-6 my-2">
        <i class="fas fa-calendar me-1"></i>
        <span>2023-02-10</span>
    </div>
    <div class="me-6 my-2">
        <i class="fas fa-clock me-1"></i>
        <span>28分钟阅读时长</span>
    </div>
    
    

    
</div>
        
        
        

        <div class="content">
            <p>22年的时候需要使用CICD的时候，看了一遍k8s相关的知识点。买了一本《深入剖析Kubernetes》，大致熟悉了常用的概念。</p>
<p>最近因为工作调动需要做云原生相关的开发，有必要重新复习一遍相关概念，并深入部分章节的细节。</p>
<h2 id="容器基础">容器基础</h2>
<h3 id="容器原理">容器原理</h3>
<p>容器是云原生时代的进程。</p>
<p>之前大家都是在ECS这种虚拟机里面部署应用，操作系统一般是CentOS/Debian之类的Linux系统，服务通过Supervisor/Systemd等进程管理工具进行管理，CPU/内存等容量规划是以虚拟机为单位的，环境一般使用bash/python脚本进行初始化，监控、服务集群扩容和缩容都是运维手动配置。</p>
<p>容器最开始是为了解决环境问题（为应用创建隔离的运行环境），有点类似Python的virtualEnv，它的底层原理是基于Linux的资源隔离机制，也就是：</p>
<ol>
<li>Namespace. 通过<code>clone</code>创建进程时，可以传入<code>CLONE_NEWPID</code>参数，此时这个进程会“看到”一个新的进程空间，在这里他的进程pid是1。在这个进程空间里，它是被隔离的，只能看到操作系统配置的东西；当然在宿主机看来这就是一个普通的进程；</li>
<li>CGroups. 用来为进程设置资源限制，如内存、CPU、磁盘、带宽等，同时还具有优先级设置、审计以及进程挂起、恢复等功能；cgroups通过文件系统实现，可以在<code>/sys/fs/cgroup</code>下看到各种资源目录，在里面建立文件夹，系统会自动创建对应的资源限制文件；将进程pid写入<code>tasks</code>即可；</li>
<li>rootfs. 用来与宿主机隔离文件系统，基于Linux的<code>pivot_root</code>/<code>chroot</code>指令；</li>
</ol>
<p>显然，宿主机上所有容器共享同一个内核。</p>
<p>Docker on Windows/Mac实际是基于kvm实现的，和linux的docker完全不同。</p>
<h3 id="镜像原理">镜像原理</h3>
<p>docker所谓的镜像，其实就是一种rootfs的压缩包，它的创新点主要是引入了层(<code>layer</code>)的概念，后者就是增量的rootfs。</p>
<p>docker通过联合挂载(union mount)技术将这些层挂载到统一的挂载点上，这通过文件系统的支持（如AuFS）来实现。</p>
<p>镜像从下到上分别是只读层、Init层和读写层：</p>
<ul>
<li>一般只读层就是操作系统镜像，如<code>ubuntu:latest</code>;</li>
<li>Init层是启动时写入容器的配置，如hostname、hosts等；</li>
<li>可读写层则是普通用户创建镜像时修改的部分；</li>
</ul>
<p>由于只读层的文件不能真正被修改或者删除，所以可读写层如果想要修改只读层的东西，aufs实际上是通过创建<code>whiteout</code>文件，把只读层的文件遮挡起来。比如<code>foo</code>文件对应的位置创建一个<code>.wh.foo</code>文件，这种屏蔽一般称为白障。只读层的挂载方式也称为<code>ro+wh</code>.</p>
<p>Docker用来制作rootfs的工具称为<code>Dockerfile</code>，例如：</p>
<pre><code class="language-dockerfile">From python:2.7-slim
WORKDIR /app
ADD . /app
RUN pip install --trusted-host pypi.python.org -r requirements.txt
EXPOSE 80
ENV NAME world
CMD [&quot;python&quot;, &quot;app.py&quot;]
</code></pre>
<p>这是一个python服务的打包过程，同目录应当有<code>app.py</code>和<code>requirements.txt</code>两个文件，进入目录，通过<code>docker build -t &lt;tag&gt; .</code>命令即可生成docker镜像。</p>
<h3 id="常用命令">常用命令</h3>
<p>使用<code>docker run -p 4000:80 helloworld</code>来运行镜像，并将镜像的80端口映射到宿主机的4000端口。</p>
<p>使用<code>docker ps</code>命令即可看到在运行的进程。如果在镜像里做了某些操作，想要把正在运行的镜像提交成一个新的镜像，可以使用<code>docker commit</code>命令。</p>
<p>可以通过<code>docker inspect --format '{{.State.Pid}}' &lt;container-id&gt;</code>命令，获取宿主机上容器的pid.在<code>/proc/&lt;pid&gt;</code>下面可以看到宿主机上该容器相关的进程文件。</p>
<p>运行<code>docker exec -it &lt;container-id&gt; /bin/sh</code>可以进行镜像的bash，进行某些操作，该操作的原理是利用了<code>setns</code>的系统调用。</p>
<p>通过<code>--net container:&lt;container-id&gt;</code>参数可以将一个容器加入另一个容器的网络空间，如果是<code>--net host</code>则直接共享宿主机的网络栈。</p>
<p>docker镜像仓库可以使用公有的docker hub，也可以使用自建的harbor(vmvare做的).</p>
<h3 id="文件系统打通">文件系统打通</h3>
<p>有时候宿主机和容器需要交换持久化数据，即文件访问。</p>
<p>Docker Volume用于将宿主机中文件挂载到容器内访问，一般通过<code>docker run</code>命令启动容器时，使用<code>-v &lt;host-path&gt;:&lt;container-path&gt;</code>传入挂载映射关系。</p>
<p>这个挂载实际上是在rootfs准备好之后，<code>chroot</code>执行之前，由<code>dockerinit</code>进程完成的。之后才是通过<code>execv</code>系统调用，让应用进程取代自己成为pid=1的容器进程。</p>
<h2 id="k8s设计概要">k8s设计概要</h2>
<img src="https://csceciti-iot-devfile.oss-cn-shenzhen.aliyuncs.com/docs/%E6%88%AA%E5%B1%8F2023-02-11%2020.48.41.png" alt="截屏2023-02-11 20.48.41" style="zoom:50%;" />
<p>k8s由master和node两类节点构成，分别称为控制节点和计算节点。</p>
<p>master由3个组件构成，control-manager负责编排，apiserver提供接口，scheduler负责容器调度；</p>
<p>计算节点核心是kubelet组件，它与下面的容器运行时（如docker）通过CRI的接口来交互；和网络层以及存储层通过CNI和CSI的接口来交互。通过这些接口的抽象，可以更换底层实际的实现，所以在k8s中docker可以随时替换成别的运行时。</p>
<p>device plugin是k8s用来调度硬件（如GPU）的插件，kubelet通过gRPC与之交互。</p>
<p>k8s源自Google内部的borg系统，但是实际上后者并没有容器镜像这种东西，而是直接用Namespace+CGroup限制应用程序。borg对于k8s的指导作用主要体现在master节点上。</p>
<p>习惯上，k8s的使用方法是：</p>
<ol>
<li>通过一个任务编排对象（如pod、cronJob等）描述你的任务对象；</li>
<li>为1中的人物定义一些运维能力对象，如Service/Ingress/HPA等；</li>
</ol>
<p>这就是所谓的声明式API，1和2中的对象又称为API对象。</p>
<h2 id="k8s集群搭建">k8s集群搭建</h2>
<p>k8s的官方推荐部署工具是<code>kubeadm</code>（还有kOPs和Kubespray），建立集群的方式非常简单：</p>
<pre><code class="language-bash"># init master
kubeadm init [--config kubeadm.yaml]
# join node to master
kubeadm join &lt;master ip:port&gt;
</code></pre>
<p>当然需要自己提前安装运行时（如containerd，如果使用DockerEngine则需要额外安装<code>cri-dockered</code>适配器），此外需要打开一些端口。master节点包括：</p>
<table>
<thead>
<tr>
<th>协议</th>
<th>方向</th>
<th>端口范围</th>
<th>目的</th>
<th>使用者</th>
</tr>
</thead>
<tbody>
<tr>
<td>TCP</td>
<td>入站</td>
<td>6443</td>
<td>Kubernetes API server</td>
<td>所有</td>
</tr>
<tr>
<td>TCP</td>
<td>入站</td>
<td>2379-2380</td>
<td>etcd server client API</td>
<td>kube-apiserver, etcd</td>
</tr>
<tr>
<td>TCP</td>
<td>入站</td>
<td>10250</td>
<td>Kubelet API</td>
<td>自身, 控制面</td>
</tr>
<tr>
<td>TCP</td>
<td>入站</td>
<td>10259</td>
<td>kube-scheduler</td>
<td>自身</td>
</tr>
<tr>
<td>TCP</td>
<td>入站</td>
<td>10257</td>
<td>kube-controller-manager</td>
<td>自身</td>
</tr>
</tbody>
</table>
<p>node节点包括：</p>
<table>
<thead>
<tr>
<th>协议</th>
<th>方向</th>
<th>端口范围</th>
<th>目的</th>
<th>使用者</th>
</tr>
</thead>
<tbody>
<tr>
<td>TCP</td>
<td>入站</td>
<td>10250</td>
<td>Kubelet API</td>
<td>自身, 控制面</td>
</tr>
<tr>
<td>TCP</td>
<td>入站</td>
<td>30000-32767</td>
<td>NodePort Services</td>
<td>所有</td>
</tr>
</tbody>
</table>
<p>NodePort默认使用30000~32767的端口，这里可以根据需要开放。截止v1.26，k8s单个集群的限制如下：</p>
<ul>
<li>每个节点的 Pod 数量不超过 110</li>
<li>节点数不超过 5,000，节点的主机名不能相同</li>
<li>Pod 总数不超过 150,000</li>
<li>容器总数不超过 300,000</li>
</ul>
<p><code>kubeadm</code>在宿主机运行kubelet，然后使用容器化部署其他组件。</p>
<p>默认情况下kube-apiserver仅支持HTTPS通信，因此kubeadm会自动生成对应的证书，路径在<code>/etc/kubernetes/pki</code>；master节点其他组件的配置文件也都在<code>/etc/kubernetes</code>下。</p>
<p>kubeadm会通过pod的方式启动其他master组件，由于此时k8s集群还不存在，这里运行pod的方式是所谓的<code>static pod</code>，其yaml路径在<code>/etc/kubernetes/manifest</code>下。</p>
<p>之后，需要安装CNI和CSI的插件，使用<code>kubectl apply -f</code>命令即可。</p>
<p>习惯上，k8s通过使用kubectl命令，与api对象（即yaml文件）进行交互。</p>
<p>API对象有<code>Metadata</code>字段，即对象的元数据，其中<code>Labels</code>用来做标记该API对象筛选依据；而<code>Annotations</code>则一般是k8s在运行过程中自动添加在API对象上的。</p>
<p>pod是k8s最小运行单元，1一个pod可以包含多个容器（使用同一个namespace）。</p>
<h2 id="常用命令-1">常用命令</h2>
<ul>
<li><code>kubectl get</code>用来获取指定的API对象，如<code>kubectl get pods -l app=nginx</code>，用来通过label过滤pods；</li>
<li><code>kubectl describe</code>用来查看对象的详细信息，如<code>kubectl describe pod &lt;pod-name&gt;</code>；</li>
<li><code>kubectl create -f &lt;name&gt;</code>生成配置文件，name可以标明对象的种类，如<code>nginx-deployment.yaml</code>；</li>
<li><code>kubectl apply -f &lt;name&gt;</code>，应用配置。k8s会自动探测这种应用是更新还是创建；</li>
<li><code>kubectl delete -f &lt;name&gt;</code>，删除API对象；</li>
<li><code>kubectl exec -it &lt;pod-name&gt; -- /bin/bash</code>，类似<code>docker exec</code>，进入pod中（容器的namespace中）；</li>
<li><code>kubectl scale deployment xxx --replicas=N</code>，水平伸缩（直接apply一个deployment也可以）；</li>
<li><code>kubectl rollout status</code>，滚动查看API对象的状态；</li>
<li><code>kubectl edit</code>直接编辑etcd中的API对象定义；</li>
<li><code>kubectl set</code>直接修改某个字段，如<code>image</code>；</li>
<li><code>kubectl patch</code>直接给API对象打补丁，补丁有具体的语法，详情可以查询文档；</li>
<li><code>kubectl logs [-f] [-p] POD [-c CONTAINER]</code>，查看pod日志；</li>
</ul>
<h2 id="k8s编排原理">k8s编排原理</h2>
<h3 id="pod原理">Pod原理</h3>
<p>Pod是一个逻辑概念，并没有对应的实体，实际操作的仍然是容器，或者说Linux Namespace和CGroup.一组共享网络Namespace的Pod，并且可以共享Volume的容器，被称为Pod.</p>
<p>所有pod，k8s都需要先创建一个<em>Infra container</em>，其他容器共享该容器的网络Namespace，这个容器由汇编语言写成，永远处于暂停状态，所以消耗的资源极少。pod的生命周期与infra容器一致，用户容器的进出流量也可以视为通过infra容器完成。</p>
<p>正是因为有了infra容器这个隐藏的container，Volume才可以定义在pod层级。</p>
<p>Pod的定义，一方面是为了调度方便（多个容器），另一方面是为了所谓的<strong>容器设计模式</strong>。也就是容器=进程，pod=进程组/虚拟机的设计。</p>
<p>通过<code>initContainers</code>，可以在pod中预定义多个容器，用来初始化环境，需要注意<code>initContainers</code>每次都会启动，如果是一次性的初始化需要检测是否需要跳过。</p>
<p>initContainers一定会先于pod启动，并按着定义的顺序严格执行；但是<strong>containers里面规定的多个容器则没有明确的启动顺序</strong>，所以如果有依赖问题，需要检测指定容器是否已经就绪。</p>
<p>多个container协作的设计模式，又称为<strong>sidecar模式</strong>。</p>
<h3 id="pod字段">Pod字段</h3>
<p>按着Pod=虚拟机这种模拟来理解，很容易明白哪些配置需要定义在pod级别，哪些是container级别。一些pod级别的常用字段（spec下面）：</p>
<ul>
<li><code>nodeSelector</code>，调度条件，如<code>diskType:ssd</code>，标明该pod只能在打有这个标签的节点上被调度。已废弃，使用<code>affinity.nodeAffinity</code>代替；</li>
<li><code>nodeName</code>，这个字段是k8s赋予的，有值的时候就会认为已调度；</li>
<li><code>hostAliases</code>，host定义；</li>
<li><code>containers</code>，容器级别的定义；</li>
<li><code>volumes</code>，卷声明；</li>
<li><code>terminationGracePeriodSeconds</code>，优雅关闭等待时长；</li>
<li><code>backoffLimit</code>，最大重启次数；</li>
<li><code>activeDeadlineSeconds</code>，最大运行市场（一般Job里面使用）；</li>
</ul>
<p>volumes声明为<code>emptyDir:{}</code>，则会在宿主机上创建一个临时目录绑定到该volume的name上；volumes也可以声明为<code>hostPath</code>，通过<code>path: /xxx</code>来直接映射宿主机的目录。这两种volume是最常用的的，但是数据卷有时候需要分布式文件系统，此时就要使用其他方式（如PVC）来声明了。</p>
<p>container级别的常用字段：</p>
<ul>
<li><code>imagePullPolicy</code>，默认是always，可以设置为<code>never</code>或者<code>ifNotPresent</code>；</li>
<li><code>lifecycle</code>，生命周期，如：<code>postStart</code>、<code>preStop</code>，用于定义容器生命周期各个阶段回调的命令；需要注意的是<code>postStart</code>启动的时候，容器的CMD可能还未结束；</li>
<li><code>livenessProbe</code>，存活探针。支持http探测（200或者其他）或者bash命令（通过返回0或者非0）探测，甚至tcp探测（端口存活）；</li>
<li><code>readinessProbe</code>，</li>
<li><code>restartPolicy</code>，重新创建pod的策略，默认是<code>Always</code>，可以设置为<code>OnFailure</code>或者<code>Never</code>。注意<code>onFailure</code>的条件是所有容器都异常了，<code>Always</code>则是任意一个容器异常；如果需要保留案发现场，则可以设置为Never；</li>
<li><code>volumeMounts</code>，卷挂载。将pod级别的卷声明根据需要挂载到容器里，通过<code>mountPath</code>指定挂载路径；</li>
<li><code>resources</code>，容器的资源声明，<code>requests</code>和<code>limits</code>来声明最小和最大限制；</li>
</ul>
<p>由于字段太多，开发人员编写API对象难度较大，此时运维人员可以预定义一些参数，这就是所谓的<code>PodPreset</code>。这也是一类API对象，他可以通过<code>spec.selector.matchLabels</code>匹配开发人员创建的API对象。注意preset需要先于pod创建。</p>
<h3 id="pod的状态">Pod的状态</h3>
<p>即pod.status.phase</p>
<ul>
<li>Pending. API对象已创建成功，但是尚未调度成功；</li>
<li>Running. API对象已调度成功；</li>
<li>Succeed. 已成功执行，一般是Job节点；</li>
<li>Failed. <strong>所有</strong>容器进程以非0状态码退出；</li>
<li>Unknown. 未知状态，apiserver无法获取pod状态，一般是主从通信出了问题；</li>
</ul>
<p>此外，还有一组conditions字段，包括：<code>PodScheduled</code>, <code>Ready</code>, <code>Initialized</code>和<code>Unschedulable</code>.两个字段需要结合来看。</p>
<h3 id="pod配置">Pod配置</h3>
<p>pod中用户容器的配置，一般通过<code>project volume</code>的方式注入进去，包括：</p>
<ul>
<li><code>Secret</code>: 将密码保存在etcd里，可以打开加密；</li>
<li><code>ConfigMap</code>: 一般的配置；</li>
<li><code>Downward API</code>: 将pod的信息暴露到容器里，必须是容器启动前就能确定下来的信息，不支持运行时更改；</li>
</ul>
<p>以上都是通过<code>kubectl create xxx</code>创建（或apply），然后通过volume挂载到pod中。当然也可以通过环境变量注入，但是后者不具备自动更新的能力。</p>
<p>ServiceAccoutToken是一种特殊的Secret，k8s默认会隐式注入到所有容器里，这样容器中的应用可以通过kubectl控制k8s.</p>
<h3 id="deployment">Deployment</h3>
<p>用于部署的API对象，pod水平伸缩滚动更新依赖于该类API对象。</p>
<p>Deployment并不是直接控制Pod对象，而是通过Replica对象来控制。</p>
<p>对于用户而言，就是通过<code>replicas</code>定义副本数量，通过<code>spec.selector.matchLabels</code>匹配pod定义。</p>
<p>然后<code>apply</code>就行，k8s会自动完成滚动更新，相关命令：</p>
<ul>
<li><code>kubectl rollout undo</code>，版本回滚；</li>
<li><code>kubectl rollout history</code>，查看版本历史；</li>
<li><code>kubectl rollout undo xxx --to-revision=N</code>，回滚到某个具体的历史版本；</li>
<li><code>kubectl rollout pause</code>和<code>kubectl rollout resume</code>，暂停部署和恢复部署；</li>
</ul>
<p>通过查看deployment对象的状态可以确定部署是否完成。</p>
<p>这里k8s的工作原理类似一个死循环，不断检查API对象的状态，如果不满足yaml中的声明，就动态调整直到满足为止。</p>
<h3 id="statefulset">StatefulSet</h3>
<p>与ReplicaSet对应的有状态Pod，主要指代两种状态：</p>
<ol>
<li>拓扑状态，即pod有严格的启动顺序，重建需要保持顺序的问题。这个statefulSet默认就能解决。解决方式是对pod赋予唯一的名称(hostname：<code>&lt;name&gt;-N</code>，N是编号)，访问pod通过headless service对应的唯一域名来访问，创建pod时保持名称不变即可；</li>
<li>存储依赖，即应用有需要落地的数据，当pod重建后能恢复这些数据；这个需要其他技术来辅助；</li>
</ol>
<p>由于Volume的编写过于复杂，所以这里又抽象出两个新概念：</p>
<ul>
<li>PVC: PersistentVolumeClaim，即对存储的需求描述，大小、挂载方式等；在volume中只要指定这个pvc就行；</li>
<li>PV：真正的volume细节，一般是运维编写；</li>
</ul>
<p>PVC例子：</p>
<pre><code class="language-yaml">spec:
  volumes:
    - name: pv-storage
      persistentVolumeClaim:
        claimName: pv-claim
</code></pre>
<p>实际使用中一般只需要说明需要的PVC：</p>
<pre><code class="language-yaml">spec:
  volumeClaimTemplate:
  - metadata:
      name: xxx
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
</code></pre>
<p>这里就指定了权限和大小，k8s会自动寻找合适的PV绑定到PVC上，PVC的名字是<code>xxx-&lt;podname&gt;</code>.</p>
<p>所以对于第2个问题，直接指定PVC即可。由于绑定到Pod的PVC的命名也有与Pod相同的编号，重建pod时找到符合该规律的PVC挂载上去即可。</p>
<p>特别注意的是，<code>StatefulSet</code>要求使用同一个容器镜像，如果容器镜像不同，需要使用<code>Operator</code>.既然是同一个镜像，那么按着pod名称中的序号0,1,2&hellip;就可知道启动的顺序，这样就可以把需要先启动的放前面。常见的需求是主从集群，0为主，1,2为从这种设计。</p>
<p>StatefulSet的滚动更新与ReplicaSet不同，有严格的销毁和启动顺序，销毁按着序号倒序进行。可以在<code>rollingUpdate</code>里按序号指定部分更新（灰度）。</p>
<h3 id="daemonset">DaemonSet</h3>
<p>宿主机守护进程，在k8s上每个节点上都会运行且只运行一个pod，当然“每个节点”不是很准确，应该是拥有满足<code>spec.selector.matchLabels</code>筛选pod的每个结点。</p>
<p>DaemonSet启动的很早，因此可以通过<code>spec.tolerations</code>指定污点来忽略某些限制。</p>
<p>类似Deployment，支持版本管理。不同的是，Deployment的一个版本对应一个ReplicaSet，DaemonSet直接控制Pod，所以实现方式不一样。</p>
<p>DaemonSet版本管理使用的对象是<code>ControllerRevision</code>， 可以通过<code>get</code>/<code>describe</code>该对象查看具体的版本信息。StateFulSet也是使用该对象实现版本管理的。</p>
<h3 id="job和cronjob">Job和CronJob</h3>
<p>一次性或者周期性调度的任务。Job直接控制Pod，CronJob则控制Job. 一些特殊字段：</p>
<ul>
<li>
<p><code>spec.parallelism</code>定义job可以启动多少个pod进行并发计算；</p>
</li>
<li>
<p><code>spec.completions</code>定义job至少完成的pod数目；</p>
</li>
<li>
<p><code>spec.concurrencypolicy</code>，cronjob对job重合的处理，默认为<code>Allow</code>，可以设置为<code>Forbid</code>或者<code>Replace</code>；</p>
</li>
</ul>
<p>CronJob的最小检查周期是10s，定义最小周期则是1分钟。</p>
<p>如果从上次运行时间到现在，CronJob创建失败次数超过了100，这个CronJob将不会再被调度；如果设置了<code>spec.startingDeadlineSeconds: n</code>，则检查的时间点就会固定到n秒之前，次数则固定是100，不可设置。这个参数还表示cronJob容许的延迟启动时间。</p>
<h3 id="声明式api">声明式API</h3>
<p>前文已经说过，尽量是用<code>kubectl apply</code>来实现CURD，而不是直接用<code>create/replace/patch</code>等命令，这是因为有并发处理同一个API对象的情况。k8s会在apply时自动处理各种冲突，最终达到需求的状态。</p>
<h3 id="自定义api资源">自定义API资源</h3>
<p>即CRD(custom resource definition)，用户可以自定义API对象，格式如下：</p>
<pre><code class="language-yaml">apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: network.samplecrd.k8s.io
spec:
  group: samplecrd.k8s.io
  version: v1
  names:
    kind: Network
    plural: networks #复数
  scope: Namespaced
</code></pre>
<p>这里需要做一些云原生开发，即golang写插件。由于涉及到编程，此处细节会单独开blog来写。</p>
<h3 id="rbac">RBAC</h3>
<p>k8s的权限控制也是基于RBAC制定的，主要概念：</p>
<ul>
<li>Role：角色，对应了具体的权限集；</li>
<li>Subject: 主题，被赋予角色的对象；</li>
<li>RoleBinding：上面两个的绑定关系；</li>
</ul>
<p>Role的定义格式：</p>
<pre><code class="language-yaml">kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: mynamespace # 必须指定namespace，默认是default
  name: example-role
rules:
  - apiGroups: [&quot;&quot;]
    resources: [&quot;pods&quot;] # 资源组
    resourceName: [&quot;mysql&quot;] # 资源的名称
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;]  # 权限动词
</code></pre>
<p>RoleBinding的定义：</p>
<pre><code class="language-yaml">kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: example-rolebinding
  namespace: mynamespace # 也必须指定namespace
subjects:
  - kind: User
    name: example-user
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: example-role
  apiGroup: rbac.authorization.k8s.io
</code></pre>
<p>如果想在所有namespace里都生效，需要使用<code>clusterRole</code>和<code>clusterRoleBindings</code>.</p>
<p>与User相比，习惯上更多使用<code>ServiceAccount</code>来分配权限，用户在pod里通过<code>spec.serviceAccountName</code>来引用该对象。</p>
<h3 id="准入控制器">准入控制器</h3>
<p>Admission Controllers，在RBAC校验通过之后，对象被持久化之前调用，主要用来过滤非只读请求。</p>
<p>可以通过webhook自定义准入控制器。</p>
<h3 id="operator">Operator</h3>
<p>由于有状态服务过于复杂，经常需要在yaml里面写逻辑。所以另外一种更编程友好的方法是使用Operator来完成。这其实就是一种CRD配合自定义控制器的完全自定义方式。</p>
<p>涉及到编程，这里不再赘述。一般Stateful搞不定的，就要使用Operator来定义了。</p>
<h2 id="k8s存储原理">k8s存储原理</h2>
<p>PV和PVC的匹配机制：</p>
<ul>
<li>PV必须满足PVC的大小限制；</li>
<li>二者的<code>storageClassName</code>必须一致（如果没声明，则为空字符串）；</li>
</ul>
<p>k8s中专门处理持久化存储的控制器叫<code>Volume Controller</code>，其维护多个控制循环，其中一个循环会持续尝试绑定PV和PVC，即<code>PersistentVolumeController</code>.所谓绑定，就是将PV对象的名字填入PVC的<code>spec.volumeName</code>字段。</p>
<h3 id="dynamic-provisioning">Dynamic Provisioning</h3>
<p>人工创建PV的方式被称为<code>Static Provisioning</code>，只能对小规模集群使用人工管理。</p>
<p>动态分配的核心在于名为<code>StorageClass</code>的API对象，其定义主要包括两个部分：</p>
<ul>
<li>PV的属性，如存储类型、Volume的大小等；</li>
<li>所需存储插件，如Ceph等，对应字段是<code>provisioner</code>；</li>
</ul>
<p>k8s官方支持存储插件很多，如果实在是不支持，也可以自己开发。</p>
<h3 id="local-pv">Local PV</h3>
<p>直接使用宿主机磁盘的PV，理论上IO性能最好，但是存在数据丢失风险。使用时需要注意：</p>
<ol>
<li>一个PV一块盘，不应当使用宿主机的主硬盘；</li>
<li>只能使用Static Provisioning，即预先创建PV才能绑定到PVC；</li>
<li>需要延迟绑定，通过<code>StorageClass.volumeBindMode: WaitForFirstConsumer</code>延迟PV和PVC的绑定；</li>
</ol>
<p>PV示例：</p>
<pre><code class="language-yaml">apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage # 存储类
  local:
    path: /mnt/disks/vol1
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions: # pod仅能在当前节点运行
          - key: kubernetes.io/hostname
            operator: In
            values:
              - node-1
</code></pre>
<p>StorageClass示例：</p>
<pre><code class="language-yaml">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer # 延迟绑定声明
</code></pre>
<p>最后删除PV的顺序：</p>
<ol>
<li>删除使用PV的Pod；</li>
<li>移除磁盘(unmount);</li>
<li>删除PVC；</li>
<li>删除PV；</li>
</ol>
<p>可以通过k8s的StaticProvisioner来自动化LocalPV的相关操作。</p>
<h3 id="csi插件编写">CSI插件编写</h3>
<p>涉及到编程，先跳过</p>
<h2 id="k8s网络原理">k8s网络原理</h2>
<p>容器的network namespace默认是隔离的，通信需要经过交换网络。</p>
<p>docker的实现方式是在宿主机创建一个名为docker0的网桥，网桥工作在二层网络，根据mac地址转发数据包。</p>
<p>然后docker为每个容器创建一对Veth Pair，这种虚拟设备成对出现，都插在docker0网桥上，一端在宿主机里，另一端在容器里(eth0)。发往任一端的数据包，在另一端都能收到，且无视namespace隔离。</p>
<p>对于跨主机网络，这个docker0就力不能及了，这时候就需要Flannel项目出马。</p>
<h3 id="flannel">Flannel</h3>
<p>该项目是一个框架，有多种后端实现，用于解决跨主机容器通信问题。</p>
<h4 id="udp后端">UDP后端</h4>
<p>flannel进程会在每个宿主机上监听一个UDP端口，并创建一个名为<code>flannel0</code>的虚拟设备，这是一个3层TUN设备，主要作用是在OS kernel层和user层之间传递ip数据包，具体来说就是将ip包在内核和flannel进程之间传递。</p>
<p>每个宿主机上的容器都属于该宿主机被分配的子网，flannel进程可以根据目标ip地址对应的子网找到其宿主机，然后将ip包转发给对应宿主机的flannel进程即可。后者会将数据包传入内核，根据路由表将数据转给docker0网桥。</p>
<p>由于数据需要多次在内核态和用户态之间切换，导致性能太低，所以已被逐渐废弃。</p>
<h4 id="vxlan后端">VXLAN后端</h4>
<p>非常类似UDP后端，VXLAN会在宿主机上设置一个特殊的网络设备<code>VTEP</code>，不过它是工作在二层。</p>
<h3 id="k8s对上述方案的兼容">k8s对上述方案的兼容</h3>
<p>k8s肯定不会直接用docker0，而是通过CNI接口来进行网络通信，默认创建的网桥是<code>cni0</code>.</p>
<p>为宿主分配子网可以使用<code>kubeadm init --pod-network-cidr=&lt;ipv4/n&gt;</code>来指定，也可以创建完成之后通过<code>kube-controller-manager</code>来指定。</p>
<p>CNI具体的配置须放在宿主机的<code>/etc/cni/net.d/</code>中，格式类似：</p>
<pre><code class="language-js">{
    &quot;name&quot;: &quot;cbr0&quot;,
    &quot;plugins&quot;:[
        {
            &quot;type&quot;: &quot;flannel&quot;,
            &quot;delegate&quot;:{ //托管给内置的插件
                &quot;hairpinMode&quot;: true, //打开发夹模式，允许自己访问自己
                &quot;isDefaultGateway&quot;: true
            }
        },
     	{
            &quot;type&quot;: &quot;portmap&quot;,
            &quot;capabilities&quot;:{
                &quot;portMappings&quot;: true,
            }
        }
    ]
}
</code></pre>
<p>容器网络相关的逻辑并不在<code>kubelet</code>中，而是在具体的CRI中，对于docker就是<code>dockershim</code>，目前已经弃用。</p>
<p>目前k8s网络不支持多个CNI复用。</p>
<p>Infra容器创建之后会执行<code>SetUpPod</code>，该函数用于为CNI插件准备参数。对于flannel，他需要的参数分为两部分：</p>
<ul>
<li>各种环境变量，其中最重要的是CNI_COMMAND，可选的值是DEL或者ADD，表示把容器添加到CNI网络，或者从中移除；</li>
<li>上面配置文件中的配置信息；</li>
</ul>
<p>参数加载完毕之后，CRI就会调用CNI插件，对于flannel而言，由于这个插件是内置在k8s网络里，所以实际上只是对配置参数做了一些补充。</p>
<p>之后，CNI插件就会调用bridge插件，后者会检查CNI网桥是否存在，没有则创建一个cni0. 之后，创建Veth Pair，将其中一端放在宿主机上，另一端放在infra容器里。</p>
<p>特别注意在infra容器的那段需要设置<code>hairpinMode: true</code>，即允许容器通过Service自己访问自己。</p>
<p>之后bridge插件调用ipam插件，为容器分配子网中某个ip地址，并绑定到容器的eth0上。</p>
<p>最后，CNI插件将ip地址这些信息返回给CRI，kubelet再将这些信息添加到Pod的相关字段上。</p>
<h3 id="host-gw后端">host-gw后端</h3>
<p>将宿主机ip作为flannel子网的下一条地址，即将宿主机当做路由器来用。这个思路其实类似物理组网。</p>
<p>优势是性能损失更低（10%左右），缺点是要求宿主机集群之间二层互通，而VXLan仅要求三层互通。一般公有云环境更推荐这种模式。</p>
<p>对于复杂项目，可以考虑使用calico项目，该项目使用BGP协议进行大规模路由表维护，避免人工维护成本。</p>
<p>由于BGP网络极其复杂，这里不再详述。</p>
<h3 id="网络隔离">网络隔离</h3>
<p>k8s通过API对象<code>NetPolicy</code>来控制网络隔离，该对象通过<code>spec.podSelector</code>来匹配pod，并进行白名单管控。</p>
<p>使用起来其实很类似IaaS中的“安全组”。</p>
<h3 id="service原理">Service原理</h3>
<p>service是由kube-proxy和iptables共同实现的。</p>
<p>推荐打开kube-proxy的IPVS模式以提高性能，纯iptables在大量pod时性能较差。</p>
<p>Service的<code>spec.type</code>支持以下几种模式：</p>
<ul>
<li>ClusterIP，最常用的的模式。如果设为None，则为headless模式，否则通过随机负载均衡访问；</li>
<li>NodePort，强行暴露pod的端口到宿主机；每个节点只能部署一个pod示例；</li>
<li>LoadBalancer，对接公有云时使用；</li>
<li>ExternalName，类似在DNS中直接加CNAME；</li>
<li>ExternalIp，直接在<code>spec.externalIps</code>里面声明可以访问的ip地址，该地址会路由到对应的service；</li>
</ul>
<p>对于4层协议，如果想获取客户端的真实ip，需要将<code>spec.externalTrafficPolicy</code>设置为local.</p>
<h3 id="ingress对象">Ingress对象</h3>
<p>即k8s对反向代理的抽象，可以视为Service的Service.</p>
<p>其实没啥好说的，配置很类似Nginx.</p>
<h2 id="k8s资源调度">k8s资源调度</h2>
<p>资源的<code>spec.resources.requests</code>表示调度需要的最小资源，<code>spec.resource.limits</code>则是CGroup设置的限制值。</p>
<p>Pod资源QoS的三个级别：</p>
<ul>
<li>Guaranteed.  同时设置上述两个指标，且二者相同；</li>
<li>Burstable. 至少有一个容器设置了requests；</li>
<li>BestEffort. 两个参数都没设置；</li>
</ul>
<p>当资源不足时，上述QoS的Pod回收的优先级是从低到高的。因此像DaemonSet这种资源，应当设置为<code>Guaranteed</code>保证尽量不被回收。</p>
<p>如果<code>Guaranteed</code>级别的Pod指定cpu数量（整数），则称为<code>cpuset</code>，指将pod绑定到具体的核上，避免频繁的上下文切换，生产环境比较常用。</p>
<h3 id="调度流程">调度流程</h3>
<p>默认调度器的调度流程分为预选和优选两步。</p>
<p>预选是并发遍历所有节点寻找满足API对象需求的Node并筛选出来，在单个节点上的筛选流程是固定的4步（顺序执行）。</p>
<p>优选则是对预选的结果分别打分，最后得分最高的被选出来。</p>
<p>不过最后创建Pod时，还是kubelet会二次检查确认满足条件。主要是因为前面的检查是无锁乐观的，最后一步还是要二次确认才能保证没问题。</p>
<h3 id="调度优先级">调度优先级</h3>
<p>默认都是0，可以创建<code>PriorityClass</code>对象来指定优先级，然后在pod的<code>spec.priorityClassName</code>里指定具体的优先级对象名字。</p>
<h3 id="device-plugin">Device Plugin</h3>
<p>cpu/内存以外的资源，都是通过<code>Extend Resource</code>来实现的。<code>Device Plugin</code>是一种插件，通过与Kubelet进行gRPC通信定时上报节点拥有的其他硬件资源。</p>
<h2 id="cri与容器运行时">CRI与容器运行时</h2>
<p>最开始k8s是基于docker实现的，后面抽象成了CRI，并逐步从核心代码中移除了dockershim.</p>
<p>容器运行时除了docker之外，现在还有containerd等项目。</p>
<p>甚至，gVistor等项目还支持硬模拟，从而创造隔离性更好的pod（如内核级别的隔离）</p>
<h2 id="k8s监控与日志">k8s监控与日志</h2>
<p>开启apiserver的aggregator模式之后，会启动一个代理。代理之后还包括metric server，这里就是k8s本身promethus格式的metric api.</p>
<p>HPA就是基于应用的metric server完成的.</p>
<p>k8s鼓励应用直接把日志直接输出到stdout和stderr，它会将日志重定向到宿主机的文件中。</p>
<h2 id="明星项目和工具">明星项目和工具</h2>
<ul>
<li>kubevela: 由于k8s配置过于复杂，对应用开发人员不太友好，该项目是为了减轻开发者负担而创建；</li>
<li>kubeclipper: kubeadm的图形化打包，方便安装集群；</li>
<li>helm: 类似apt的包管理工具，直接在k8s里面安装容器；</li>
<li>k3s: 简化的k8s，边缘机器使用；配合docker的叫做k3d；</li>
<li>kind: 利用docker快速部署集群，可以在本机开发时使用；完全兼容k8s；</li>
<li>krew: kubectl的插件管理工具；</li>
<li>Lens：多集群管理；</li>
<li>Capsule/vCluster：单集群多租户管理工具；后者更成熟；</li>
<li>SchemaHero：云原生的数据库迁移工具；</li>
</ul>
<h2 id="k8s国内安装">k8s国内安装</h2>
<p>可以参考这个<a href="https://github.com/mingcheng/deploy-k8s-within-aliyun-mirror">repo</a>，主要使用阿里云的镜像来安装，当然完整的流程还是要参考<a href="https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/">k8s官方流程</a>。</p>
<p>update: 建议使用<a href="https://github.com/DaoCloud/public-image-mirror">DaoCloud</a>的工具来安装，更加无侵入和傻瓜化。</p>
<p>由于装的时候踩了不少坑，这里还是记录一下详细流程。</p>
<h3 id="准备工作">准备工作</h3>
<p>linux配置：</p>
<ol>
<li>关闭swap，注意是永久关闭，不要临时关闭，否则重启之后kubelet运行不了：</li>
</ol>
<pre><code class="language-bash">sed -ri 's/.*swap.*/#&amp;/' /etc/fstab
swapoff  -a
</code></pre>
<ol start="2">
<li>
<p>配置静态ip，如果是云端服务，需要购买弹性ip；</p>
</li>
<li>
<p>配置内核参数，先启用对应的内核模块(ubuntu可能需要先用apt install bridge-utils)：</p>
</li>
</ol>
<pre><code class="language-bash"> modprobe overlay
 modprobe br_netfilter
</code></pre>
<p>上面是临时修改。如果要永久修改，不同发行版不太一样，ubuntu只要修改<code>/etc/modules</code>，在里面写入上面两个模块的名字即可。</p>
<p>然后编辑<code>/etc/sysctl.conf</code>，加入：</p>
<pre><code class="language-ini">net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1

net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1

net.ipv4.ip_forward = 1
net.ipv6.conf.all.forwarding = 0
net.bridge.bridge-nf-call-ip6tables = 0

vm.swappiness = 0
</code></pre>
<p>这个配置里面禁用了ipv6，如果想要启用双栈，则应参考<a href="https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/dual-stack-support/">这里</a>，一般应该是不用启用的。修改之后运行<code>sysctl -p</code>生效。</p>
<ol start="4">
<li>
<p>安装容器运行时，为了适配v1.24之后的k8s，这里不再安装docker，仅安装containerd。参考<a href="https://github.com/containerd/containerd/blob/main/docs/getting-started.md">官方文档</a>安装即可，如果使用apt/dnf安装，需要将CNI插件手动下载并解压到指定位置；</p>
</li>
<li>
<p>配置containerd，需要配置地方比较多，主要是：</p>
</li>
</ol>
<pre><code class="language-toml">  [plugins.&quot;io.containerd.grpc.v1.cri&quot;]
    sandbox_image = &quot;registry.aliyuncs.com/google_containers/pause:3.6&quot;

      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]
      endpoint = [&quot;https://mirror.baiduce.com&quot;,&quot;https://dockerproxy.com&quot;]
      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;k8s.gcr.io&quot;]
      endpoint = [&quot;registry.aliyuncs.com/google_containers&quot;]

    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc]
      ...
      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options]
        SystemdCgroup = true
</code></pre>
<ol start="5">
<li>安装kubelet/kubectl和kubeadm，这里必须用阿里云镜像了，参考<a href="https://developer.aliyun.com/mirror/kubernetes/">这里</a>，修改apt/yum的配置，然后安装<strong>指定版本</strong>并冻结版本即可。</li>
<li>使用<code>systemctl enable containerd &amp;&amp; systemctl start containerd</code>启动运行时；kubelet可能也需要类似操作；</li>
<li>集群中的多台主机的主机名不能重复；</li>
<li>如果是搭建多master的HA模式，需要配置nginx/haproxy做LB，或者使用云主机商提供的LB；</li>
</ol>
<h3 id="安装master">安装master</h3>
<p>以上准备工作完成后，就可以用kubeadm进行安装了，配置文件参考（v1.23）：</p>
<pre><code class="language-yaml">apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
#这里改成你的实际ip地址
  advertiseAddress: 172.16.20.14
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock
  imagePullPolicy: Always
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
# 这里同样需要改成实际ip，如果有LB，则设为LB的入口地址（可以是域名）
controlPlaneEndpoint: 172.16.20.14:6443
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
# 改为你需要的版本
kubernetesVersion: v1.23.16
networking:
  # 这里必须和flannel的配置一致，用其他CNI实现则需要参考插件的描述
  podSubnet: &quot;10.244.0.0/16&quot;
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
# 节点pod限制可以参考自己的机器配置
maxPods: 200
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: &quot;ipvs&quot;
ipvs:
  strictARP: true
</code></pre>
<p>根据需求修改上面的配置，之后跑<code>kubeadm init --config xxx.yaml</code>用上面的文件初始化控制面.</p>
<p>根据提示操作，root/非root用户都需要配置一下。</p>
<p>运行<code>kubectl get nodes</code>，确认一切正常了。</p>
<p>如果配置有误，可以<code>kubeadm reset</code>重置，然后重新init.</p>
<h3 id="配置插件">配置插件</h3>
<p>配置CNI：</p>
<pre><code class="language-bash">kubectl apply -f https://ghproxy.com/https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
</code></pre>
<p>直接跑flannel的yml即可.</p>
<p>完成之后要看flannel能否正常work：</p>
<pre><code>kubectl get pods -n kube-flannel
</code></pre>
<p>配置CSI，不过这一步是可选，如果有集群可以考虑使用Rook+Ceph，否则使用host-path也够了。</p>
<p>master节点默认禁止调度用户pod，可以通过移除taint取消限制（for 1.24之前，之后的可以移除掉NoSchdule）：</p>
<pre><code>kubectl taint nodes --all node-role.kubernetes.io/master-
</code></pre>
<p>helm的安装需要翻墙，建议直接下载二进制文件上传过去。chart可以用微软的仓库，或者华为的：</p>
<pre><code>helm repo add microsoft http://mirror.azure.cn/kubernetes/charts/
</code></pre>
<h3 id="安装ingress">安装ingress</h3>
<p>如果自己测试的话用NodePort模式就够了，稍微大一点规模的微服务，一般都是用ingress的.</p>
<p>ingress的安装其实也是<code>kubectl apply -f</code>，问题是这个镜像要从Google拉，所以不能直接用。将yaml下载到本地，替换<code>registry.k8s.io</code>为<code>k8s.m.daocloud.io</code>，然后再apply即可。</p>
<p>后面需要配置Nginx，跟传统的其实差不多，只是upstream是动态的cluster domain.</p>

        </div>
        
        <div class="my-4">
    
    <a href="https://yiuterran.github.io/blog/tags/k8s/" class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 me-2 hover:text-eureka">#k8s</a>
    
</div>
        
        
        


        
        
        <div class="py-2">
    
    <div class="flex flex-col md:flex-row items-center my-8">
        <a href="https://yiuterran.github.io/blog/authors/tryao/" class="w-24 h-24 md:me-4">
            
            
            <img src="https://yiuterran.github.io/blog/images/avatar.png" class="w-full bg-primary-bg rounded-full" alt="Avatar">
            
        </a>
        <div class="w-full md:w-auto mt-4 md:mt-0">
            <a href="https://yiuterran.github.io/blog/authors/tryao/" class="block font-bold text-lg pb-1 mb-2 border-b">个人介绍</a>
            <span class="block pb-2">兴趣使然的程序员，博而不精，乐学不倦</span>
            
            
            
            
            
            <a href="mailto:yaotairan@gmail.com" class="me-1">
                <i class="fas fa-envelope"></i>
            </a>
            
            
            
            
            
            <a href="https://github.com/YiuTerran" class="me-1">
                <i class="fab fa-github"></i>
            </a>
            
        </div>
    </div>
    
</div>
        
        
        
<div class="flex flex-col md:flex-row md:justify-between -mx-2 mt-4 px-2 pt-4 border-t">
    <div>
        
        <span class="block font-bold">上一页</span>
        <a href="https://yiuterran.github.io/blog/posts/kubeedge%E8%A6%81%E7%82%B9%E7%AC%94%E8%AE%B0/" class="block">KubeEdge要点笔记</a>
        
    </div>
    <div class="md:text-right mt-4 md:mt-0">
        
        <span class="block font-bold">下一页</span>
        <a href="https://yiuterran.github.io/blog/posts/makefile%E8%A6%81%E7%82%B9%E7%AE%80%E8%AE%B0/" class="block">Makefile要点简记</a>
        
    </div>
</div>

        



  <script id="utterances" src="https://utteranc.es/client.js"
            issue-term=title
            repo=YiuTerran/blog-comment
              theme=preferred-color-scheme
        crossorigin="anonymous"
        async>
</script>
<script>
    if (storageColorScheme == "Light") {
      document.getElementById('utterances').setAttribute('theme', 'github-light')
    } else if (storageColorScheme == "Dark") {
      document.getElementById('utterances').setAttribute('theme', 'github-dark')
    }
</script>

    </div>
    
    <div class="col-span-2">
        
        
        <div class="sticky top-16 z-10 hidden lg:block px-6 py-4  bg-primary-bg ">
    <span class="text-lg font-semibold">本页内容</span>
</div>
<div class="sticky-toc hidden lg:block px-6 pb-6 ">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#容器基础">容器基础</a>
      <ul>
        <li><a href="#容器原理">容器原理</a></li>
        <li><a href="#镜像原理">镜像原理</a></li>
        <li><a href="#常用命令">常用命令</a></li>
        <li><a href="#文件系统打通">文件系统打通</a></li>
      </ul>
    </li>
    <li><a href="#k8s设计概要">k8s设计概要</a></li>
    <li><a href="#k8s集群搭建">k8s集群搭建</a></li>
    <li><a href="#常用命令-1">常用命令</a></li>
    <li><a href="#k8s编排原理">k8s编排原理</a>
      <ul>
        <li><a href="#pod原理">Pod原理</a></li>
        <li><a href="#pod字段">Pod字段</a></li>
        <li><a href="#pod的状态">Pod的状态</a></li>
        <li><a href="#pod配置">Pod配置</a></li>
        <li><a href="#deployment">Deployment</a></li>
        <li><a href="#statefulset">StatefulSet</a></li>
        <li><a href="#daemonset">DaemonSet</a></li>
        <li><a href="#job和cronjob">Job和CronJob</a></li>
        <li><a href="#声明式api">声明式API</a></li>
        <li><a href="#自定义api资源">自定义API资源</a></li>
        <li><a href="#rbac">RBAC</a></li>
        <li><a href="#准入控制器">准入控制器</a></li>
        <li><a href="#operator">Operator</a></li>
      </ul>
    </li>
    <li><a href="#k8s存储原理">k8s存储原理</a>
      <ul>
        <li><a href="#dynamic-provisioning">Dynamic Provisioning</a></li>
        <li><a href="#local-pv">Local PV</a></li>
        <li><a href="#csi插件编写">CSI插件编写</a></li>
      </ul>
    </li>
    <li><a href="#k8s网络原理">k8s网络原理</a>
      <ul>
        <li><a href="#flannel">Flannel</a>
          <ul>
            <li><a href="#udp后端">UDP后端</a></li>
            <li><a href="#vxlan后端">VXLAN后端</a></li>
          </ul>
        </li>
        <li><a href="#k8s对上述方案的兼容">k8s对上述方案的兼容</a></li>
        <li><a href="#host-gw后端">host-gw后端</a></li>
        <li><a href="#网络隔离">网络隔离</a></li>
        <li><a href="#service原理">Service原理</a></li>
        <li><a href="#ingress对象">Ingress对象</a></li>
      </ul>
    </li>
    <li><a href="#k8s资源调度">k8s资源调度</a>
      <ul>
        <li><a href="#调度流程">调度流程</a></li>
        <li><a href="#调度优先级">调度优先级</a></li>
        <li><a href="#device-plugin">Device Plugin</a></li>
      </ul>
    </li>
    <li><a href="#cri与容器运行时">CRI与容器运行时</a></li>
    <li><a href="#k8s监控与日志">k8s监控与日志</a></li>
    <li><a href="#明星项目和工具">明星项目和工具</a></li>
    <li><a href="#k8s国内安装">k8s国内安装</a>
      <ul>
        <li><a href="#准备工作">准备工作</a></li>
        <li><a href="#安装master">安装master</a></li>
        <li><a href="#配置插件">配置插件</a></li>
        <li><a href="#安装ingress">安装ingress</a></li>
      </ul>
    </li>
  </ul>
</nav>
</div>
<script>
    window.addEventListener('DOMContentLoaded', () => {
        enableStickyToc();
    });
</script>
        
    </div>
    

    
    
</div>
<script>
    document.addEventListener('DOMContentLoaded', ()=>{
        hljs.initHighlightingOnLoad();
    })
</script>

      </div>
    </div>
    
  </main>
  <footer class="ps-scrollbar">
    <div class="w-full max-w-screen-xl mx-auto"><div class="text-center p-6 pin-b">
    <p class="text-sm text-tertiary-text">&copy; 2021 <a href="https://github.com/YiuTerran">tryao</a>
 &middot;  Powered by the <a href="https://github.com/wangchucheng/hugo-eureka" class="hover:text-eureka">Eureka</a> theme for <a href="https://gohugo.io" class="hover:text-eureka">Hugo</a></p>
</div></div>
  </footer>
</body>

</html>